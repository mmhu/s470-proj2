---
title: "Small Project 2"
author: "Syaza Senin, Eungkoo (Eugene) Kahng, Miriam Hu, Youngihn Kwon"
date: "November 3, 2017"
output:
  pdf_document: default
---

```{r, include=FALSE}
library(knitr)
library(MASS)
library(tidyverse)
library(vcd)
library(RColorBrewer)
opts_chunk$set(echo = FALSE,
               cache = TRUE, autodep = TRUE,
               message = FALSE, warning = FALSE)
```

## Question 1

```{r data-loading}
test <- read.table("http://www.stat.ufl.edu/~winner/data/concussion.dat")
colnames(test) <- c("gender", "sport", "year", "concussion", "count")

con1 <- subset(test, concussion == 1)
con2 <- subset(test, concussion == 0)
con1$count0 <- con2$count
con1$prop <- con1$count /  (con1$count + con1$count0)
total.count <- aggregate(count ~ concussion, test, sum)
```


#### i)


```{r logit-reg, fig.align = "center"}
test.logit <- glm(cbind(count, count0) ~ gender + sport + factor(year), 
             family = binomial(link='logit'), data = con1)

test.logit.df <- con1
test.logit.df$.fitted <- fitted.values(test.logit)
test.logit.df$.resid <- residuals(test.logit, type = "response")
ggplot(test.logit.df, aes(x = .fitted, y = .resid)) + geom_point() +
  geom_smooth(method = "loess", method.args = list(degree = 1))

test.fitted <- sort(fitted.values(test.logit)) - mean(fitted.values(test.logit))
test.residuals <- sort(residuals(test.logit))
n <- length(test.residuals)
f.value <- (0.5:(n - 0.5)) / n
test.fit <- data.frame(f.value, Fitted = test.fitted, Residuals = test.residuals)
test.fit.long <- test.fit %>% gather(type, value, Fitted:Residuals)
ggplot(test.fit.long, aes(x = f.value, y = value)) + geom_point() +
  facet_wrap(~type) + ggtitle("Residual vs Fitted Graph")
```

We are not adding any interaction because sports and gender interaction still gives you the straight-ish line for fitted plots, so the interaction term does not give us any additional information. In terms of the residual and fitted, the loess fitting is doing a good job capturing the residual pattern, which does not seem to deviate much from 0.


#### ii)

```{r error-rate}
null.test.logit <- glm(cbind(count, count0) ~ 1, family = binomial(link='logit'),
                       data = con1)
null.probs <- predict(null.test.logit, type = "response")
test.probs <- predict(test.logit, type = "response")
error.rate.null = mean(null.probs)
error.rate.fitted = mean(test.probs)
cbind(error.rate.null, error.rate.fitted)
```

a). Each error rate of our fitted model was below 0.05 and the mean of error rate from our fitted model was 0.000806. We get low error rates because the proportion of people who get concussions is extremely low. The counts for no concussion have magnitude $10^4$, while the counts for concussion are fewer than 100. So the denominator is large, making the error rate small. We cannot conclude that our model is useful just because of the low error rate. For example, if we predicted "no concussion" for every single subject, the error rate would still be low, but predicting no every time is obviously useless. We would need another measure (such as cross validation) to find out how useful the model is.

b). The mean of error rate from our fitted model was 0.000806 and the mean of error rate from null model was 0.000746. Both error rate was close to zero and the error rate from two models was equal as 0.0008. Again, because the incidence of getting a concussion is so low in the first place, our error rate is low when we predict that most people will not get a concussion for the logistic model using our predictors and the null model. So, our model is not necessarily useless.

## Question 2

#### i)

```{r count-plots, fig.align = "center"}
ggplot(test, aes(x = sport, y = count, fill = gender)) + geom_bar(stat = "identity") +
  facet_wrap(~year) + xlab("Sports") + theme(axis.text.x = element_text(angle = 90, hjust = 1))

test.table <- xtabs(count ~ gender + sport + year, data = test)
mosaic(~year + sport + gender, test.table, highlighting="year", highlighting_fill = brewer.pal(3, "Set3"))
```


#### ii)

##### (a)

```{r poisson-data}
test.new <- test[test$concussion == 1, ]
Mean <- mean(test.new$count)
Variance <- var(test.new$count)
```

```{r pois-reg, echo = T}
test.poisson <- glm(count ~ gender + sport + year, family = poisson, data = test.new)
```

```{r overdispersion, fig.align = "center"}
test.fitted <- fitted.values(test.poisson)
test.resid <- residuals(test.poisson, type = "response")
test.std.resid <- test.resid / sqrt(test.fitted)
Overdispersion <- sum(test.std.resid^2) / df.residual(test.poisson)

data.frame(Mean, Variance, Overdispersion)
```

Here we have overdispersion value as `r Overdispersion`, which is over 1 but pretty close to 1. Thus, this Poisson model with the data is arguably correct. Also, the Poisson model does not allow variance to be different from the mean.

##### (b)

```{r refit, fig.align = "center"}
test$sport <- relevel(test$sport, ref = "Lacrosse")
test.poisson1 <- glm(count ~ sport + gender + year, family = poisson, data = test)

coefficients(summary(test.poisson1))[1:7, 1:2]

sport.co <- coefficients(summary(test.poisson1))[1:5, 1:2]
sports <- c("Basketball", "Gymnastics", "Soccer", "Softball/Baseball")
estimate <- exp(sport.co[2:5, 1])
a <- qpois(0.05, mean(test.new$count))
lower <- exp(sport.co[2:5, 1] - a * sport.co[2:5, 2])
upper <- exp(sport.co[2:5, 1] + a * sport.co[2:5, 2])
sport.co.df <- data.frame(sports, estimate, lower, upper)
ggplot(sport.co.df, aes(x = sports, y = estimate, ymin = lower, ymax = upper)) + 
  geom_pointrange() + geom_abline(intercept = 1, slope = 0, color = "red") + 
  scale_y_continuous(breaks=seq(0, 6, 0.5)) + 
  ggtitle("Approximate 95% confidence intervals for concussion count rates \n lacrosse as baseline") +
  coord_flip()

test$sport <- relevel(test$sport, ref = "Gymnastics")
test.poisson1 <- glm(count ~ sport + gender + year, family = poisson, data = test)

coefficients(summary(test.poisson1))[1:7, 1:2]

sport.co <- coefficients(summary(test.poisson1))[1:5, 1:2]
sports <- c("Basketball", "Lacrosse", "Soccer", "Softball/Baseball")
estimate <- exp(sport.co[2:5, 1])
a <- qpois(0.05, mean(test.new$count))
lower <- exp(sport.co[2:5, 1] - a * sport.co[2:5, 2])
upper <- exp(sport.co[2:5, 1] + a * sport.co[2:5, 2])
sport.co.df <- data.frame(sports, estimate, lower, upper)
ggplot(sport.co.df, aes(x = sports, y = estimate, ymin = lower, ymax = upper)) + 
  geom_pointrange() + geom_abline(intercept = 1, slope = 0, color = "red")  + 
  ggtitle("Approximate 95% confidence intervals for concussion count rates \n gymnastics as baseline") + coord_flip()
```

We basically see When comparing the two cases where lacrosse or gymnastics is the baseline, we have a a better idea of ratio relative to the baseline to use lacrosse rather than gymnastics because it is clear to detect the 95% CI indicates narrower widths.
